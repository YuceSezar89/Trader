import asyncio
import json
import logging
import os
from logging.handlers import RotatingFileHandler
from typing import Dict, List, Optional

import pandas as pd

from binance.websocket.um_futures.websocket_client import UMFuturesWebsocketClient

from binance_client import BinanceClientManager
from utils.exceptions import BinanceAPIError
from indicators.core import add_all_indicators
from database.crud import (bulk_insert_price_data,
                           get_last_timestamp, initialize_database, delete_symbol_data)
from signals.signal_processor import process_and_enrich_signals
from utils.exceptions import BinanceAPIError, DatabaseError
from config import Config
from utils.redis_client import RedisClient

def setup_logging():
    """live_data_manager iÃ§in Ã¶zel loglama ayarlarÄ±nÄ± yapar."""
    log_dir = Config.LOG_DIR
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    logger = logging.getLogger(__name__)
    logger.setLevel(Config.LOG_LEVEL)
    logger.propagate = False  # Root logger'a loglarÄ±n gitmesini engelle

    # Handler'larÄ±n tekrar tekrar eklenmesini Ã¶nle
    if logger.hasHandlers():
        logger.handlers.clear()

    # Dosya Handler'Ä± (Rotating)
    file_handler = RotatingFileHandler(
        os.path.join(log_dir, 'live_data_manager.log'),
        maxBytes=Config.LOG_FILE_MAX_SIZE,
        backupCount=Config.LOG_FILE_BACKUP_COUNT
    )
    # Konsol Handler'Ä±
    stream_handler = logging.StreamHandler()

    formatter = logging.Formatter(Config.LOG_FORMAT, datefmt=Config.LOG_DATE_FORMAT)
    file_handler.setFormatter(formatter)
    stream_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

    return logger

# --- Logging Kurulumu ---
logger = setup_logging()


class LiveDataManager:
    """
    Tarihsel verileri senkronize eden ve ardÄ±ndan WebSocket Ã¼zerinden canlÄ± veri alarak
    sinyal Ã¼reten yÃ¶netici sÄ±nÄ±fÄ±.
    """

    def __init__(self, symbols: List[str], interval: str = Config.KLINE_INTERVAL):
        self.ref_symbol = Config.MARKET_REFERENCE_SYMBOL
        # Referans sembolÃ¼n listede olduÄŸundan emin ol
        if self.ref_symbol not in symbols:
            symbols.insert(0, self.ref_symbol) # BaÅŸa ekle
        self.symbols = list(dict.fromkeys(symbols)) # Duplike varsa kaldÄ±r

        self.interval = interval
        # WebSocket istemcisi: tek baÄŸlantÄ± Ã¼zerinde combined-stream kullanacaÄŸÄ±z
        self.ws_client = UMFuturesWebsocketClient(
            on_message=self._handle_websocket_message,
            on_close=self._handle_ws_close,
            on_error=self._handle_ws_error
        )
        self.is_ws_connected = False
        self.last_message_time: Optional[float] = None  # Son WebSocket mesajÄ±nÄ±n zamanÄ±nÄ± takip et
        self.reconnect_attempt = 0  # Ãœstel backoff iÃ§in sayaÃ§
        self.db_lock = asyncio.Lock()  # VeritabanÄ± yazma iÅŸlemleri iÃ§in kilit
        self.kline_data: Dict[str, pd.DataFrame] = {symbol: pd.DataFrame() for symbol in symbols}
        self.processing_tasks: set[asyncio.Task] = set()
        try:
            self.loop = asyncio.get_running_loop()
        except RuntimeError:
            self.loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self.loop)

    async def sync_historical_data(self):
        """
        TÃ¼m semboller iÃ§in geÃ§miÅŸ verileri senkronize eder.
        - Her sembol iÃ§in veritabanÄ±ndan son zaman damgasÄ±nÄ± alÄ±r.
        - Binance'ten son zaman damgasÄ±ndan bu yana eksik olan mumlarÄ± Ã§eker.
        - Ã‡ekilen verileri veritabanÄ±na kaydeder.
        """
        logger.info("Tarihsel veri senkronizasyonu baÅŸlatÄ±lÄ±yor...")
        tasks = [self._sync_symbol_data(symbol) for symbol in self.symbols]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for symbol, result in zip(self.symbols, results):
            if isinstance(result, Exception):
                logger.error(f"[{symbol}] Tarihsel veri senkronizasyonu sÄ±rasÄ±nda kritik hata: {result}")
            else:
                logger.info(f"[{symbol}] Tarihsel veri senkronizasyonu tamamlandÄ±.")

    async def _sync_symbol_data(self, symbol: str):
        """Helper method to sync historical data for a single symbol."""
        try:
            last_timestamp = await get_last_timestamp(symbol, interval=self.interval)
            start_time = last_timestamp + 1 if last_timestamp else None

            if start_time:
                # start_time bir sonraki ms olduÄŸundan, gÃ¶rÃ¼ntÃ¼leme iÃ§in 1 ms geri alÄ±yoruz
                logger.info(f"[{symbol}] Son kayÄ±t: {pd.to_datetime(start_time - 1, unit='ms')}. Eksik veriler Ã§ekiliyor...")
            else:
                logger.info(f"[{symbol}] VeritabanÄ±nda kayÄ±t bulunamadÄ±. Son 1500 mum Ã§ekiliyor...")

            df_missing = await BinanceClientManager.fetch_klines(
                symbol=symbol,
                interval=self.interval,
                limit=1500,
                startTime=start_time
            )

            if not df_missing.empty:
                logger.info(f"[{symbol}] {len(df_missing)} adet yeni mum verisi bulundu. VeritabanÄ±na kaydediliyor...")
                # Ä°ndikatÃ¶r hesaplamadan doÄŸrudan ham veriyi kaydet
                async with self.db_lock:
                    await bulk_insert_price_data(symbol, df_missing, interval=self.interval)
            else:
                logger.info(f"[{symbol}] Yeni veri bulunamadÄ±, sistem gÃ¼ncel.")

        except DatabaseError as e:
            logger.error(f"[{symbol}] VeritabanÄ± hatasÄ± oluÅŸtu: {e}")
            raise
        except BinanceAPIError as e:
            logger.error(f"[{symbol}] Veri senkronizasyonu sÄ±rasÄ±nda Binance API hatasÄ±: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"[{symbol}] Veri senkronizasyonunda beklenmedik hata: {e}", exc_info=True)
            raise

    async def _initialize_dataframes(self):
        """Initializes in-memory DataFrames with the last 500 klines for signal calculation."""
        logger.info("Sinyal hesaplamasÄ± iÃ§in baÅŸlangÄ±Ã§ verileri yÃ¼kleniyor...")
        tasks = [self._load_initial_data(symbol) for symbol in self.symbols]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        symbols_to_remove = []
        for symbol, result in zip(self.symbols, results):
            if isinstance(result, Exception):
                logger.error(f"[{symbol}] BaÅŸlangÄ±Ã§ verisi yÃ¼klenirken hata: {result}")
                symbols_to_remove.append(symbol)
            elif isinstance(result, pd.DataFrame) and not result.empty:
                # Son 24 saatlik (96 * 15dk) veride hacim kontrolÃ¼
                recent_data = result.tail(96)
                if recent_data['volume'].sum() < Config.MIN_VOLUME_THRESHOLD:
                    logger.warning(f"[{symbol}] DÃ¼ÅŸÃ¼k hacimli (son 24s hacim < {Config.MIN_VOLUME_THRESHOLD}), izlemeden Ã§Ä±karÄ±lÄ±yor.")
                    symbols_to_remove.append(symbol)
                    # Bu sembol iÃ§in veritabanÄ±ndan da temizlik yapalÄ±m
                    task = asyncio.create_task(self._purge_symbol_data(symbol))
                    self.processing_tasks.add(task)
                    task.add_done_callback(self.processing_tasks.discard)
                else:
                    df = add_all_indicators(result)
                    self.kline_data[symbol] = df
                    logger.info(f"[{symbol}] {len(df)} adet mum baÅŸlangÄ±Ã§ verisi olarak yÃ¼klendi ve gÃ¶stergeler hesaplandÄ±.")
            else:
                logger.warning(f"[{symbol}] iÃ§in baÅŸlangÄ±Ã§ verisi yÃ¼klenemedi veya veri boÅŸ, izlemeden Ã§Ä±karÄ±lÄ±yor.")
                symbols_to_remove.append(symbol)

        if symbols_to_remove:
            self.symbols = [s for s in self.symbols if s not in symbols_to_remove]
            for s in symbols_to_remove:
                del self.kline_data[s]
            logger.info(f"DÃ¼ÅŸÃ¼k hacimli/hatalÄ± semboller temizlendi. GÃ¼ncel izleme listesi: {self.symbols}")

    async def _load_initial_data(self, symbol: str) -> pd.DataFrame:
        """Helper to fetch initial kline data for one symbol."""
        try:
            # We fetch 500 to have enough data for indicators like MA200
            return await BinanceClientManager.fetch_klines(symbol, self.interval, limit=500)
        except BinanceAPIError as e:
            logger.error(f"[{symbol}] BaÅŸlangÄ±Ã§ verisi Ã§ekilirken Binance API hatasÄ±: {e}", exc_info=True)
            raise # HatayÄ± yukarÄ±ya ilet
        except Exception as e:
            logger.error(f"[{symbol}] BaÅŸlangÄ±Ã§ verisi Ã§ekilemedi: {e}", exc_info=True)
            raise # HatayÄ± yukarÄ±ya ilet

    def _handle_websocket_message(self, _, msg: str):
        """WebSocket'ten gelen her mesajÄ± iÅŸler."""
        self.last_message_time = self.loop.time() # Her mesajda zamanÄ± gÃ¼ncelle
        try:
            data = json.loads(msg)
            if data.get('e') == 'kline':
                kline = data['k']
                symbol = kline['s']
                is_closed = kline['x']

                if is_closed:
                    logger.info(f"ğŸ•¯ï¸ [{symbol}] Mum kapandÄ±. Fiyat: {kline['c']}")
                    # WebSocket thread'inden ana event loop'a gÃ¼venli coroutine Ã§aÄŸrÄ±sÄ±
                    asyncio.run_coroutine_threadsafe(self._update_and_process_symbol(symbol, kline), self.loop)

        except json.JSONDecodeError:
            logger.error(f"WebSocket'ten bozuk JSON verisi alÄ±ndÄ±: {msg}")
        except Exception as e:
            logger.error(f"WebSocket mesaj iÅŸleme hatasÄ±: {e} | Mesaj: {msg}", exc_info=True)

    async def _update_and_process_symbol(self, symbol: str, kline_data: Dict):
        """Updates the DataFrame with the new kline and triggers signal processing."""
        try:
            new_row = {
                'open_time': int(kline_data['t']),
                'open': float(kline_data['o']),
                'high': float(kline_data['h']),
                'low': float(kline_data['l']),
                'close': float(kline_data['c']),
                'volume': float(kline_data['v']),
                'close_time': int(kline_data['T']),
                'quote_asset_volume': float(kline_data['q']),
                'number_of_trades': int(kline_data['n']),
                'taker_buy_base_asset_volume': float(kline_data['V']),
                'taker_buy_quote_asset_volume': float(kline_data['Q']),
            }
            
            # Using pd.concat instead of append
            new_df = pd.DataFrame([new_row])
            self.kline_data[symbol] = pd.concat([self.kline_data[symbol], new_df], ignore_index=True)

            # Bellekteki veri setini belirli bir boyutta tut (Ã¶rneÄŸin son 1000 mum)
            self.kline_data[symbol] = self.kline_data[symbol].tail(1000)

            # Ã–nce yeni kapanan mumu (ham veri) veritabanÄ±na kaydet
            try:
                async with self.db_lock:
                    await bulk_insert_price_data(symbol, new_df, interval=self.interval)
                logger.info(f"[{symbol}] Yeni kapanan mum (ham veri) veritabanÄ±na kaydedildi.")
            except Exception as db_err:
                logger.error(f"[{symbol}] CanlÄ± veri veritabanÄ±na kaydedilirken hata: {db_err}", exc_info=True)

            # HafÄ±zadaki veri Ã¼zerinde sinyal Ã¼retimi iÃ§in gÃ¶stergeleri hesapla
            self.kline_data[symbol] = add_all_indicators(self.kline_data[symbol])

            # GÃ¶stergelerle zenginleÅŸtirilmiÅŸ DataFrame'i Redis'e yaz
            # Yeni standart anahtar: <prefix>:<symbol>:<interval>
            new_redis_key = f"{Config.REDIS_LIVE_DATA_KEY_PREFIX}:{symbol}:{self.interval}"
            await RedisClient.set_df(new_redis_key, self.kline_data[symbol])
            # Geriye uyum: eski anahtara da yaz (yakÄ±nda kaldÄ±rÄ±labilir)
            legacy_redis_key = f"{Config.REDIS_LIVE_DATA_KEY_PREFIX}:{symbol}"
            await RedisClient.set_df(legacy_redis_key, self.kline_data[symbol])
            logger.info(f"[{symbol}] CanlÄ± veri Redis'e yazÄ±ldÄ±. Yeni: {new_redis_key} | Legacy: {legacy_redis_key}")

            task = asyncio.create_task(self._process_signal_for_symbol(symbol))
            self.processing_tasks.add(task)
            task.add_done_callback(self.processing_tasks.discard)

        except Exception as e:
            logger.error(f"Failed to update and process symbol {symbol}: {e}", exc_info=True)

    async def _purge_symbol_data(self, symbol: str):
        """Deletes all data for a given symbol from the database."""
        try:
            # from database.crud import delete_symbol_data # ArtÄ±k gerekli deÄŸil, global scope'a taÅŸÄ±nacak.
            logger.info(f"[{symbol}] VeritabanÄ±ndan temizleniyor...")
            async with self.db_lock:
                await delete_symbol_data(symbol)
            logger.info(f"[{symbol}] VeritabanÄ±ndan baÅŸarÄ±yla temizlendi.")
        except Exception as e:
            logger.error(f"[{symbol}] VeritabanÄ± temizliÄŸi sÄ±rasÄ±nda hata: {e}")

    def _handle_ws_close(self, _):
        """Callback function for when the websocket connection is closed."""
        logger.warning("WebSocket baÄŸlantÄ±sÄ± kapandÄ±.")
        self.is_ws_connected = False

    def _handle_ws_error(self, _, error):
        """Callback function for websocket errors."""
        logger.error(f"WebSocket hatasÄ±: {error}", exc_info=True)
        self.is_ws_connected = False

    async def _process_signal_for_symbol(self, symbol: str):
        """Belirli bir sembol iÃ§in sinyal hesaplamasÄ±nÄ± ve zenginleÅŸtirmesini tetikler."""
        try:
            if symbol == self.ref_symbol:
                return  # Referans sembol iÃ§in sinyal Ã¼retme

            df = self.kline_data.get(symbol)
            ref_df = self.kline_data.get(self.ref_symbol)

            if df is None or ref_df is None or df.empty or ref_df.empty:
                logger.warning(f"[{symbol}] Sinyal iÅŸleme iÃ§in yeterli veri bulunamadÄ±, atlanÄ±yor.")
                return

            # Kilit mekanizmasÄ±nÄ± `process_and_enrich_signals` fonksiyonuna devretmek yerine burada yÃ¶netebiliriz.
            # Ancak `create_signal` zaten kendi iÃ§inde atomik olmalÄ±. Åimdilik kilitsiz devam edelim.
            # async with self.db_lock:
            await process_and_enrich_signals(
                symbol=symbol,
                df=df.copy(),
                ref_df=ref_df.copy(),
                interval=self.interval
            )
        except Exception as e:
            logger.error(f"Sinyal iÅŸleme ana hatasÄ± - {symbol}: {e}", exc_info=True)

    async def start_streams(self):
        """Starts the WebSocket streams for all symbols using a single combined stream."""
        if not self.symbols:
            logger.warning("Ä°zlenecek sembol kalmadÄ±, WebSocket baÅŸlatÄ±lmÄ±yor.")
            return

        logger.info(f"WebSocket yayÄ±nlarÄ± baÅŸlatÄ±lÄ±yor: {self.symbols}")
        # Combined stream (tek baÄŸlantÄ±) iÃ§in stream listesi
        streams = [f"{s.lower()}@kline_{self.interval}" for s in self.symbols]
        
        # Yeni bir ws istemcisi oluÅŸturmak genellikle daha temiz bir yeniden baÄŸlanma saÄŸlar
        self.ws_client = UMFuturesWebsocketClient(
            on_message=self._handle_websocket_message,
            on_close=self._handle_ws_close,
            on_error=self._handle_ws_error
        )
        # Not: binance-futures-connector, bir liste geÃ§ildiÄŸinde combined stream aÃ§ar (tek WS).
        # Ping/pong iÃ§in client internal ayarlarÄ± kullanÄ±r; watchdog'u biz Ã¼stten izliyoruz.
        self.ws_client.subscribe(stream=streams, id=1)
        self.is_ws_connected = True
        self.reconnect_attempt = 0  # BaÅŸarÄ±lÄ± baÄŸlantÄ±da backoff'u sÄ±fÄ±rla
        logger.info("TÃ¼m WebSocket yayÄ±nlarÄ±na baÅŸarÄ±yla abone olundu.")

    async def run(self):
        """Ana Ã§alÄ±ÅŸtÄ±rma dÃ¶ngÃ¼sÃ¼."""
        try:
            # 1. Eksik geÃ§miÅŸ verileri tamamla
            await self.sync_historical_data()
            # 2. Sinyal hesaplamasÄ± iÃ§in hafÄ±zadaki DataFrame'leri ilk verilerle doldur
            await self._initialize_dataframes()
            # 3. CanlÄ± veri akÄ±ÅŸÄ±nÄ± baÅŸlat (bu bloklamaz)
            await self.start_streams()
            
            logger.info("CanlÄ± veri yÃ¶neticisi Ã§alÄ±ÅŸÄ±yor. BaÄŸlantÄ± izleniyor... Ã‡Ä±kmak iÃ§in CTRL+C.")

            # BaÅŸlangÄ±Ã§ta son mesaj zamanÄ±nÄ± ayarla
            if self.is_ws_connected:
                self.last_message_time = self.loop.time()

            while True:
                reconnect_reason = None
                if not self.is_ws_connected:
                    reconnect_reason = "WebSocket baÄŸlantÄ±sÄ± koptu."
                elif self.last_message_time and (self.loop.time() - self.last_message_time) > Config.WEBSOCKET_TIMEOUT:
                    reconnect_reason = f"WebSocket zaman aÅŸÄ±mÄ±na uÄŸradÄ± ({Config.WEBSOCKET_TIMEOUT}s)."

                if reconnect_reason:
                    # Ãœstel backoff + jitter
                    backoff_base = getattr(Config, 'WS_RECONNECT_BACKOFF_BASE', 5)
                    backoff_max = getattr(Config, 'WS_RECONNECT_BACKOFF_MAX', 60)
                    delay = min(backoff_max, backoff_base * (2 ** self.reconnect_attempt))
                    # Basit jitter: +/- 20%
                    jitter = max(1.0, delay * 0.2)
                    import random
                    sleep_for = max(1.0, delay + random.uniform(-jitter, jitter))
                    logger.warning(f"{reconnect_reason} {sleep_for:.1f} saniye iÃ§inde yeniden baÄŸlanma denenecek... (attempt={self.reconnect_attempt})")
                    self.is_ws_connected = False  # Yeniden baÄŸlanma sÃ¼recini baÅŸlatmak iÃ§in
                    await asyncio.sleep(sleep_for)
                    try:
                        # Eski istemciyi durdurmayÄ± dene, ancak takÄ±lÄ±rsa devam et
                        if self.ws_client:
                           await asyncio.to_thread(self.ws_client.stop)
                        
                        logger.info("Yeni WebSocket baÄŸlantÄ±sÄ± kuruluyor...")
                        await self.start_streams()
                        # Yeniden baÄŸlandÄ±ktan sonra zamanÄ± sÄ±fÄ±rla
                        if self.is_ws_connected:
                           self.last_message_time = self.loop.time()
                           self.reconnect_attempt = 0
                           logger.info("WebSocket baÄŸlantÄ±sÄ± baÅŸarÄ±yla yeniden kuruldu.")
                        else:
                           self.reconnect_attempt += 1
                           logger.error("Yeniden baÄŸlanma denemesi baÅŸarÄ±sÄ±z oldu.")

                    except Exception as e:
                        logger.error(f"WebSocket yeniden baÅŸlatma sÄ±rasÄ±nda kritik hata: {e}", exc_info=True)
                        self.reconnect_attempt += 1
                        # Hata durumunda da backoff uygulanÄ±r
                        backoff_base = getattr(Config, 'WS_RECONNECT_BACKOFF_BASE', 5)
                        backoff_max = getattr(Config, 'WS_RECONNECT_BACKOFF_MAX', 60)
                        delay = min(backoff_max, backoff_base * (2 ** self.reconnect_attempt))
                        jitter = max(1.0, delay * 0.2)
                        import random
                        sleep_for = max(1.0, delay + random.uniform(-jitter, jitter))
                        logger.info(f"{sleep_for:.1f} saniye sonra tekrar denenecek.")
                        await asyncio.sleep(sleep_for)
                else:
                    # BaÄŸlantÄ± saÄŸlamsa, dÃ¶ngÃ¼yÃ¼ tÄ±kamadan bekle
                    # Ping/Pong watchdog: belirli aralÄ±kla heartbeat kontrolÃ¼
                    await asyncio.sleep(getattr(Config, 'WS_HEARTBEAT_CHECK_INTERVAL', 5))
        except asyncio.CancelledError:
            logger.info("Ana Ã§alÄ±ÅŸtÄ±rma dÃ¶ngÃ¼sÃ¼ iptal edildi.")
        finally:
            await self.shutdown()

    async def shutdown(self):
        """TÃ¼m gÃ¶revleri ve servisleri dÃ¼zgÃ¼nce kapatÄ±r."""
        logger.info("Kapatma iÅŸlemi baÅŸlatÄ±lÄ±yor...")
        # WebSocket istemcisini durdur (non-blocking)
        if self.ws_client:
            await asyncio.to_thread(self.ws_client.stop)
        logger.info("WebSocket istemcisi durduruldu.")

        # Sadece bizim oluÅŸturduÄŸumuz iÅŸlem gÃ¶revlerini iptal et
        tasks = list(self.processing_tasks)
        if tasks:
            logger.info(f"{len(tasks)} adet bekleyen gÃ¶rev iptal ediliyor...")
            for task in tasks:
                task.cancel()
            
            await asyncio.gather(*tasks, return_exceptions=True)
            logger.info("TÃ¼m bekleyen gÃ¶revler baÅŸarÄ±yla iptal edildi.")
        else:
            logger.info("Ä°ptal edilecek bekleyen gÃ¶rev bulunamadÄ±.")

async def main():
    """UygulamanÄ±n ana giriÅŸ noktasÄ±."""
    # VeritabanÄ±nÄ± ve tablolarÄ± oluÅŸtur
    await initialize_database()

    logger.info("En yÃ¼ksek hacimli semboller Binance'ten Ã§ekiliyor...")
    symbols_to_track = await BinanceClientManager.get_top_volume_symbols_async(limit=Config.SYMBOL_LIMIT)
    # Referans sembolÃ¼n izleme listesinde olduÄŸundan emin ol
    if Config.MARKET_REFERENCE_SYMBOL not in symbols_to_track:
        symbols_to_track.insert(0, Config.MARKET_REFERENCE_SYMBOL)
    logger.info(f"{len(symbols_to_track)} adet sembol bulundu.")

    if not symbols_to_track:
        logger.error("Ä°zlenecek sembol bulunamadÄ±. Binance API veya baÄŸlantÄ± sorunu olabilir.")
        return

    manager = LiveDataManager(symbols=symbols_to_track, interval=Config.KLINE_INTERVAL)
    main_task = asyncio.create_task(manager.run())

    try:
        await main_task
    except asyncio.CancelledError:
        logger.info("Ana gÃ¶rev (main_task) iptal edildi.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ Program kullanÄ±cÄ± tarafÄ±ndan sonlandÄ±rÄ±ldÄ±.")
